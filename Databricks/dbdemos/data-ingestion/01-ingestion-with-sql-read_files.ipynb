{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182a9a7d-e46e-4ce7-a31b-8ae60a8c2906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# What is Databricks SQL's read_files function?\n",
    "The `read_files` function lets you directly query and ingest files (like `CSV`, `JSON`, `Parquet`, etc.) from cloud storage or Unity Catalog using SQL—no table setup needed.\n",
    "\n",
    "It supports ad-hoc exploration and incremental ingestion, including with `STREAMING TABLES`, and automatically infers schema and handles directories or patterns.\n",
    "\n",
    "*Note: Lakeflow Declarative Pipelines use read_files to easily load file data into tables, powering both batch and streaming workflows.*\n",
    "\n",
    "For more details, open [the read_files documentation](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c562c57d-5c34-4502-808b-fa21dee088ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data initialization - run the cell to prepare the demo data."
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebe2e49-6718-4079-b7a3-0ba226ca80ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(volume_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd386ab1-e2d1-4644-905a-baf68833ace4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1. Basic Usage: Automatic Format Dectection\n",
    "\n",
    "One of the key advantages of `read_files` is automatic format detection. Let's try to read many file formats in our demo data folder, and use read_files to detect the file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7873376b-6da0-4197-9d84-5ff075579385",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading JSON files (auto-detected)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_json') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b04e17c-f5bb-474e-9081-63378f4a269d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading CSV files (auto-detected)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_csv') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d513366-f731-4793-8387-f7990afdc3b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading parquet files (auto-detected)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_parquet') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998f6459-96a6-4307-b7fa-e76779fd730f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Partitioned Parquet auto-detection"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT year, month, COUNT(*) as records FROM read_files('{volume_folder}/user_parquet_partitioned') GROUP BY year, month\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7ab9980-15c5-4de6-8070-59ef0c65583b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "`read_files` also supports powerful glob patterns for selective file reading. You can select the specific format you want to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "670ab396-cdaf-43f8-9a06-a7c70362b721",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Basic glob patterns"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT 'JSON Files' as source, * FROM read_files('{volume_folder}/*json*') LIMIT 3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21b0ed65-0afa-432f-8759-116636ea8520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2. Schema Inference\n",
    "\n",
    "Different formats have different schema inference capabilities and performance.\n",
    "\n",
    "We can also use schema hints to override the schema inferrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4a1b19-4344-4768-ab42-d0d1f1b072fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON schema (inferred from data)"
    }
   },
   "outputs": [],
   "source": [
    "json_schema = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_json') LIMIT 0\").schema\n",
    "print(json_schema.treeString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6fac31c-f1e6-4b2f-9f98-6f609798f5d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV schema (inferred with headers)"
    }
   },
   "outputs": [],
   "source": [
    "csv_schema = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_csv') LIMIT 0\").schema  \n",
    "print(csv_schema.treeString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e72f0e-5773-4b71-9474-8a3add180774",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Type Precision Comparison"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  format,\n",
    "  MAX(id_type) AS id_type,\n",
    "  MAX(age_group_type) AS age_group_type,\n",
    "  MAX(date_type) AS date_type\n",
    "FROM (\n",
    "SELECT \n",
    "  'JSON' as format,\n",
    "  typeof(id) as id_type,\n",
    "  typeof(age_group) as age_group_type,\n",
    "  typeof(creation_date) as date_type\n",
    "FROM read_files('{volume_folder}/user_json')\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'CSV' as format,\n",
    "  typeof(id) as id_type, \n",
    "  typeof(age_group) as age_group_type,\n",
    "  typeof(creation_date) as date_type\n",
    "FROM read_files('{volume_folder}/user_csv')\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'Parquet' as format,\n",
    "  typeof(id) as id_type,\n",
    "  typeof(age_group) as age_group_type, \n",
    "  typeof(creation_date) as date_type\n",
    "FROM read_files('{volume_folder}/user_parquet')\n",
    ") type_comparision\n",
    "GROUP BY format\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e01a97-9878-46aa-9d85-86885269a5ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using schema hints to override JSON inference"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  id,\n",
    "  typeof(id) as id_type_after_hint,\n",
    "  age_group,\n",
    "  typeof(age_group) as age_group_type_after_hint\n",
    "FROM read_files(\n",
    "  '{volume_folder}/user_json',\n",
    "  schemaHints => 'id bigint, age_group string'\n",
    ") LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7210b1a8-1b69-40be-85ca-0f58a0b96cc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3. Format-Specific Features\n",
    "\n",
    "There are some particular options that are specific to each format with `read_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3470696-aaf2-496f-93b7-6a998107f935",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV without headers"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_csv_no_headers', format => 'csv', header => 'false') LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01735aa4-0117-465a-8bed-8b102a4de983",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV without headers (manual schema)"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT * FROM read_files(\n",
    "  '{volume_folder}/user_csv_no_headers',\n",
    "  format => 'csv',\n",
    "  schema => 'id bigint, creation_date string, firstname string, lastname string, email string, address string, gender double, age_group double'\n",
    ") LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da15b5f-745f-45ce-a17b-cb4b8ddf1a9a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV with pipe delimiter"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT * FROM read_files(\n",
    "  '{volume_folder}/user_csv_pipe_delimited',\n",
    "  format => 'csv',\n",
    "  sep => '|'  \n",
    ") LIMIT 5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398369c9-0799-4db2-bb5e-ff659378d04c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON with column type inference"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  firstname,\n",
    "  lastname,\n",
    "  id,\n",
    "  typeof(id) as id_inferred_type,\n",
    "  age_group,\n",
    "  typeof(age_group) as age_group_inferred_type\n",
    "FROM read_files(\n",
    "  '{volume_folder}/user_json',\n",
    "  inferColumnTypes => true\n",
    ") LIMIT 5  \n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85193149-6217-48eb-a4ed-6e9eac7042d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parquet optimization features"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstrate column pruning (Parquet's key advantage)\n",
    "print(\"⚡ Parquet Column Pruning Demo:\")\n",
    "import time\n",
    "\n",
    "# Read all columns\n",
    "start_time = time.time()\n",
    "all_cols_count = spark.sql(f\"SELECT * FROM read_files('{volume_folder}/user_parquet')\").count()\n",
    "all_cols_time = time.time() - start_time\n",
    "\n",
    "# Read only specific columns  \n",
    "start_time = time.time()\n",
    "select_cols_count = spark.sql(f\"SELECT id, firstname FROM read_files('{volume_folder}/user_parquet')\").count()\n",
    "select_cols_time = time.time() - start_time\n",
    "\n",
    "print(f\"📊 All columns: {all_cols_count:,} records in {all_cols_time:.2f}s\")\n",
    "print(f\"📊 2 columns: {select_cols_count:,} records in {select_cols_time:.2f}s\") \n",
    "print(f\"⚡ Column pruning speedup: {all_cols_time/select_cols_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d980f5f8-649e-47f0-875c-33febdcfae0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Streaming Usage\n",
    "\n",
    "`read_files` can be used in streaming tables to ingest files into Delta Lake. `read_files` leverages Auto Loader when used in a streaming table query.\n",
    "\n",
    "To do so, simply add the `STREAM` keyword to your SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c21e5b3-0d36-4e04-a0c9-275c82a22931",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Basic streaming example"
    }
   },
   "outputs": [],
   "source": [
    "# Create a streaming view\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW streaming_json_users AS\n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() as processing_time\n",
    "FROM STREAM read_files(\n",
    "  '{volume_folder}/user_json',\n",
    "  maxFilesPerTrigger => 5,\n",
    "  schemaLocation => '{volume_folder}/read_files_streaming_schema'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "display(spark.sql(\"SELECT COUNT(*) as total_records FROM streaming_json_users\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac9fd0fb-9a23-443a-9961-881244ecbabb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Multi-format streaming (separate streams with schema locations)"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW streaming_csv_users AS  \n",
    "SELECT \n",
    "  *,\n",
    "  'CSV' as source_format,\n",
    "  current_timestamp() as processing_time\n",
    "FROM STREAM read_files(\n",
    "  '{volume_folder}/user_csv',\n",
    "  schemaLocation => '{volume_folder}/csv_streaming_schema'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW streaming_parquet_users AS\n",
    "SELECT \n",
    "  *,\n",
    "  'PARQUET' as source_format, \n",
    "  current_timestamp() as processing_time\n",
    "FROM STREAM read_files(\n",
    "  '{volume_folder}/user_parquet',\n",
    "  schemaLocation => '{volume_folder}/parquet_streaming_schema'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "display(spark.sql(\"\"\"\n",
    "SELECT 'CSV' as format, COUNT(*) as records FROM streaming_csv_users\n",
    "UNION ALL  \n",
    "SELECT 'PARQUET' as format, COUNT(*) as records FROM streaming_parquet_users\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd0a7a8-841b-419f-8c3a-160835412a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 5. read_files vs Auto Loader\n",
    "\n",
    "We have covered some basic features of `read_files`. However, there might be some questions about when to use `read_files` and when to use Auto Loader.\n",
    "\n",
    "We have some comparison and decision matrix that could help you decide when to leverage the power of `read_files` and Auto Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c547ec0-9fef-439e-904b-47a5b03559d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "| Capability | read_files | Auto Loader |\n",
    "|-----------|------------|-------------|\n",
    "| Language | SQL | Python  |\n",
    "| Ad-hoc queries | ✅ Perfect | Incremental Streaming focus  |\n",
    "| Batch processing | ✅ Excellent | Incremental Streaming focus |\n",
    "| Multi-format API | ✅ Unified API | Need to declare format |\n",
    "| Streaming performance | Optimized for you | Mode advanced options for more control |\n",
    "| Schema evolution | ⚠️ Manual | ✅ Automatic |\n",
    "| Setup complexity | ✅ Zero setup | Pythonic config |\n",
    "| File notifications | ❌ No | ✅ Cloud notifications |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43a08602-0571-49ec-8c9f-4ef0f566fea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "We have seen what the capabilities of Databricks SQL's `read_files` are, and now you can apply it in your projects.\n",
    "\n",
    "Open the [02-Auto-loader-schema-evolution-Ingestion]($./02-Auto-loader-schema-evolution-Ingestion) Notebook to explore the Auto Loader options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8edef6e-1671-449a-bd26-5d0fb79984db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-ingestion-with-sql-read_files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
