{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "f379e6d8-fbbb-4c43-85b4-aaedcdaab9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Ingesting data from any sources with Databricks Data Intelligence Platform\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/di_platform_0.png\" style=\"float: left; margin-right: 30px\" width=\"600px\" />\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "With the Databricks Data Intelligence Platform, you can effortlessly ingest data from virtually any source, unifying your entire data estate into a single, intelligent foundation. Whether it's batch, streaming, or change data capture (CDC), we provide robust, scalable, and easy-to-use tools to bring all your data into the Lakehouse.\n",
    "\n",
    "Key capabilities include:\n",
    "\n",
    "- **Universal Connectivity**: Native connectors for enterprise applications (Salesforce, ServiceNow, Workday), databases (SQL Server, Oracle, PostgreSQL), cloud object storage (S3, ADLS, GCS), messaging queues (Kafka, Kinesis, Pub/Sub), and custom APIs.\n",
    "\n",
    "- **Automated & Incremental Ingestion**: Leverage powerful features like Auto Loader for efficient, incremental processing of new files as they arrive, and Lakeflow Connect for managed, serverless pipelines.\n",
    "\n",
    "- **Real-time Ready**: Seamlessly ingest and process high-throughput streaming data for immediate insights, real-time dashboards, and instant decision-making.\n",
    "\n",
    "- **Unified Governance**: Every ingested dataset is automatically governed by Unity Catalog, providing unified visibility, access control, lineage, and discovery across your entire data and AI assets.\n",
    "\n",
    "- **Simplified Data Engineering**: Build and manage data ingestion pipelines with ease using declarative frameworks like Lakeflow Declarative Pipelines (formely known as DLT), reducing complexity and accelerating time to value.\n",
    "\n",
    "Break down data silos, accelerate your data and AI initiatives, and unlock the full potential of your data with the Databricks Data Intelligence Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "a130b7d7-102a-4bd1-8a99-fc111f020c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1/ Ingest from Business Applications with Lakeflow connect\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/product/data-ingestion/lakeflow-connect.png\" style=\"float: left; margin-right: 30px; margin-top: 30px; margin-bottom: 30px;\" width=\"600px\" />\n",
    "\n",
    "**Databricks Lakeflow Connect**\n",
    "Lakeflow Connect is Databricks Lakeflow’s ingestion tool that makes it easy to bring data from many sources—including apps like Salesforce, Workday, and ServiceNow, plus databases, cloud storage, and streaming services—into your Databricks Lakehouse.\n",
    "\n",
    "* Features an intuitive UI for fast setup\n",
    "* Supports incremental and scalable, serverless ingestion\n",
    "* Unifies governance with Unity Catalog for security and discoverability\n",
    "\n",
    "Want to try Lakeflow Connect? Check these Product Tours:\n",
    "\n",
    "[**Ingest from Salesforce:**](https://app.getreprise.com/launch/BXZjz8X/) Seamlessly integrates Salesforce CRM data (including custom objects and formula fields) with Databricks for analytics and AI.\n",
    "\n",
    "[**Ingest Workday Reports:**](https://app.getreprise.com/launch/ryNY32X/) Quickly ingest and manage Workday data, as well as databases, cloud, and local files, with simple pipeline configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e2bad9-467c-493b-810e-ab0f29194ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2/ Read or ingest data in SQL with `read_file` and Lakeflow Declarative Pipelines\n",
    "\n",
    "### Instantly Access Any File with Databricks SQL's `read_files`\n",
    "\n",
    "**Unlock your data's potential, no matter where it lives or what format it's in.**\n",
    "\n",
    "The `read_files` function in Databricks SQL empowers you to directly query and analyze raw data files—from CSVs and JSONs to Parquet and more—stored in your cloud object storage or Unity Catalog volumes. Skip the complex setup and jump straight into insights.\n",
    "\n",
    "**Simply point, query, and transform.** `read_files` intelligently infers schemas, handles diverse file types, and integrates seamlessly with streaming tables for real-time ingestion. It's the fast, flexible way to bring all your files into the Databricks Lakehouse, accelerating your journey from raw data to actionable intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b34af5-69c8-4747-a9e0-9d2fba9f4182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Open [01-ingestion-with-sql-read_files]($./01-ingestion-with-sql-read_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484b8c78-86c1-44a3-9254-8d2262363b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3/ Ingest Files with Databricks Auto Loader\n",
    "\n",
    "**Databricks Auto Loader** makes file ingestion simple and automatic. It incrementally detects and loads new data files from cloud storage into your Lakehouse—no manual setup required.\n",
    "\n",
    "- **Fully automated streaming:** New data is picked up and processed instantly as it arrives.\n",
    "- **Exactly-once data integrity:** Ensures files are processed just once, even after failures.\n",
    "- **Supports schema evolution:** Automatically adapts as your data evolves.\n",
    "- **Works with many file formats:** Handles JSON, CSV, Parquet, Avro, and more.\n",
    "- **Seamless integration:** Built to work with Delta Live Tables and streaming pipelines.\n",
    "\n",
    "Auto Loader lets you focus on getting insights from data—not on managing file arrivals or pipeline code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9df74c1-ce11-467c-b365-785abbe0da8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Open [02-Auto-loader-schema-evolution-Ingestion]($./02-Auto-loader-schema-evolution-Ingestion)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-Ingestion-data-introduction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
