{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be962ecc-de60-4862-a595-705b4bfbf57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploring the raw data\n",
    "\n",
    "For this demo, we've generated some data and stored it in a Unity Catalog Volume. Volumes can store any type of file and can either be managed by Unity Catalog or connected to cloud storage. Lakeflow Declaritive Pipelines can automatically pick up new files and incrementally process data in a volume making your pipelines fast and efficient.\n",
    "\n",
    "Let's start by taking a look at the contents of the `raw_data` volume.\n",
    "\n",
    "**Note: this notebook is a simple Exploration Notebook, it's not part of our final Pipeline!**\n",
    "\n",
    "Having a notebook on the side to test SQL queries interactively can be very handy to accelerate exploration and build your pipelines faster!\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=796524194907820&notebook=%2Fexplorations%2F01-Exploring-the-Data&demo_name=pipeline-bike&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fpipeline-bike%2Fexplorations%2F01-Exploring-the-Data&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b4d76f-e76d-4d48-bb0a-0adaa4ef82a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "raw_data_volume = \"/Volumes/main/dbdemos_pipeline_bike/raw_data/\"\n",
    "\n",
    "# Print out a list of directories in our raw_data volume and a few files from those directories\n",
    "for table in os.listdir(raw_data_volume):\n",
    "  print(table + \"/\")\n",
    "  for file in os.listdir(raw_data_volume + table)[:3]:\n",
    "    print(\"  \" + file)\n",
    "  print(\"  ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24cc105c-aee7-4fed-a9db-f2b88b566adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It looks like we've got a few directories here with `csv` and `json` files in them. Let's start by taking a look at the maintenance logs files using the SQL `read_files` function.\n",
    "\n",
    "`read_files` supports several different file formats including `csv` and `json`. Take a look at the [Databricks documentation](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files) to see the available formats and options.\n",
    "\n",
    "Additionally, using the `STREAM` keyword `read_files` can be used in streaming tables to ingest files into Delta Lake. `read_files` leverages Auto Loader when used in a streaming table query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a24eaf4e-3938-43bf-9c2e-c513bafac00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/main/dbdemos_pipeline_bike/raw_data/maintenance_logs/*.csv\", format => \"csv\") limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b1338f-d7c0-4276-9d0d-0068ab09cf4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These files contains the field `issue_description` which is a free text field people can use to enter in a description of the issue they ran into while using a bike. Free text fields often include character sequences that may break CSV parsers. Let's do some data exploration on this data to see if we are processing it correctly.\n",
    "\n",
    "Based on our knowledge of the system giving us this data, all the fields are required. Let's look at records where that's not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed7608b4-9c38-417a-9fcf-057975e985e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Yup, it looks like there's some instances where the `issue_description` fields include a newline character. We can use `multiline => true` to tell `read_files` that records may span multiple lines and see if that fixes the issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca830d2c-3c96-465d-a57e-87395af2301a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/main/dbdemos_pipeline_bike/raw_data/maintenance_logs/*.csv\", format => \"csv\", multiline => true)\n",
    "where maintenance_id is null or bike_id is null or reported_time is null or resolved_time is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58121246-d764-43e2-906f-027252ba4d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's do some quick spot checks on the `ride_logs` and `weather` files. The files in the `weather` directory are `json` files, so we need to make sure to use the `json` format option in `read_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0eeccf-0068-44cc-a6dd-b50e2c0429ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/main/dbdemos_pipeline_bike/raw_data/rides/*.csv\", format => \"csv\") limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7cc532a-3a2b-4551-8137-747955aaf704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/main/dbdemos_pipeline_bike/raw_data/weather/*.json\", format => \"json\") limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b365853-5d94-4bc5-82b9-2a6902c35c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/main/dbdemos_pipeline_bike/raw_data/customers_cdc/*.parquet\", format => \"parquet\") limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a912c169-20f1-47e5-a574-186c6c036c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Understanding Change Data Capture (CDC) with AUTO CDC\n",
    "\n",
    "AUTO CDC is a declarative API in Lakeflow Declarative Pipelines that simplifies change data capture processing. \n",
    "\n",
    "Key benefits of AUTO CDC:\n",
    "- **Automatic ordering**: Handles records that arrive out of chronological order\n",
    "- **Built-in SCD support**: Easily implement Type 1 or Type 2 slowly changing dimensions\n",
    "- **Declarative syntax**: Simple SQL-based configuration without complex merge logic\n",
    "- **Operation handling**: Supports INSERT, UPDATE, DELETE, and TRUNCATE operations\n",
    "\n",
    "Let's explore the distribution of CDC operations to understand the types of changes happening to customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1edd5172-4071-4737-872b-44bab3498fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select \n",
    "  operation,\n",
    "  count(*) as count,\n",
    "  round(count(*) * 100.0 / sum(count(*)) over(), 1) as percentage\n",
    "from read_files(\"/Volumes/main/dbdemos_pipeline_bike/raw_data/customers_cdc/*.parquet\", format => \"parquet\") \n",
    "group by operation\n",
    "order by count desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "253b0a7e-85b8-4846-be9a-dd829dd68efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Next: Building our Declarative Pipeline\n",
    "We now have a good idea of our raw data and the queries we'll have to do!\n",
    "\n",
    "It's time to start building our pipeline!\n",
    "\n",
    "If you want to know more about Streaming Tables and Materialized views, open the [00-pipeline-tutorial notebook]($../transformations/00-pipeline-tutorial).\n",
    "\n",
    "If you know what you're doing, feel free to jump to the [01-bronze.sql]($../transformations/01-bronze.sql), [02-silver.sql]($../transformations/02-silver.sql) or [03-gold.sql]($../transformations/03-gold.sql) file!\n",
    "\n",
    "### Alternative: learn how to track your Declarative Pipeline data quality\n",
    "\n",
    "Lakeflow Declarative Pipelines makes it easy to track your data quality and set alerts when something is wrong! Open the [02-Pipeline-event-monitoring]($./02-Pipeline-event-monitoring) notebook for more details."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5317486578662967,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Exploring-the-Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
