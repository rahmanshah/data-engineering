{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d162fa-3c35-4b4e-bb5d-8b57cb65a754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "major, minor = sys.version_info[:2]\n",
    "assert (major, minor) >= (3, 11), f\"This demo expect python version 3.11, but found {major}.{minor}. \\nUse DBR15.4 or above. \\nIf you're on serverless compute, open the 'Environment' menu on the right of your notebook, set it to >=2 and apply.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cdf43a9-7fde-4d3a-a1e1-2ebb503cd348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e10b7a9-0c3c-4f98-b38a-613950d8f943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-global-setup-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a8e576-98be-4371-912a-ba2194c4c548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.setup_schema(catalog, db, False, volume_name)\n",
    "volume_folder =  f\"/Volumes/{catalog}/{db}/{volume_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b3fdc41-20d3-4a81-91f8-599ef44f57fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for f in dbutils.fs.ls(volume_folder):\n",
    "  dbutils.fs.rm(f.path, True)  \n",
    "#dbutils.fs.rm(volume_folder, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ae93a29-2527-4d9c-bb83-8b9e270897e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To speedup the demo, we won't generate the data but download it directly. IF it fails you can run the generation notebooks in the _resource folders instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc1b31d3-4f16-4d4d-80e5-8308002d9d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_exists = False\n",
    "try:\n",
    "  dbutils.fs.ls(volume_folder)\n",
    "  dbutils.fs.ls(volume_folder+\"/customers\")\n",
    "  dbutils.fs.ls(volume_folder+\"/subscriptions\")\n",
    "  dbutils.fs.ls(volume_folder+\"/billing\")\n",
    "  dbutils.fs.ls(volume_folder+\"/eval_dataset\")\n",
    "  dbutils.fs.ls(volume_folder+\"/pdf_documentation\")\n",
    "  data_exists = True\n",
    "  print(\"data already exists\")\n",
    "except Exception as e:\n",
    "  print(f\"folder doesn't exists, generating the data...\")\n",
    "\n",
    "\n",
    "if not data_exists:\n",
    "    try:\n",
    "        DBDemos.download_file_from_git(volume_folder+'/customers', \"databricks-demos\", \"dbdemos-dataset\", \"/llm/ai-agent/customers\")\n",
    "        DBDemos.download_file_from_git(volume_folder+'/subscriptions', \"databricks-demos\", \"dbdemos-dataset\", \"/llm/ai-agent/subscriptions\")\n",
    "        DBDemos.download_file_from_git(volume_folder+'/billing', \"databricks-demos\", \"dbdemos-dataset\", \"/llm/ai-agent/billing\")\n",
    "        DBDemos.download_file_from_git(volume_folder+'/eval_dataset', \"databricks-demos\", \"dbdemos-dataset\", \"/llm/ai-agent/eval_dataset\")\n",
    "        DBDemos.download_file_from_git(volume_folder+'/pdf_documentation', \"databricks-demos\", \"dbdemos-dataset\", \"/llm/ai-agent/pdf_documentation\")\n",
    "        data_downloaded = True\n",
    "    except Exception as e: \n",
    "        print(f\"Error trying to download the file from the repo: {str(e)}. Will generate the data instead...\")    \n",
    "else:\n",
    "    data_downloaded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3d46984-26d1-4bf4-ac0e-f0356c849bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(\"customers\") or \\\n",
    "    not spark.catalog.tableExists(\"subscriptions\") or \\\n",
    "    not spark.catalog.tableExists(\"billing\") :\n",
    "        spark.read.parquet(f\"{volume_folder}/customers\").write.mode('overwrite').saveAsTable('customers')\n",
    "        spark.read.parquet(f\"{volume_folder}/subscriptions\").write.mode('overwrite').saveAsTable('subscriptions')\n",
    "        spark.read.parquet(f\"{volume_folder}/billing\").write.mode('overwrite').saveAsTable('billing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "163c537c-0d13-4d83-85b4-e08a7d9b3c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#helper function defined to log the model and wrap\n",
    "def log_customer_support_agent_model(resources, request_example):\n",
    "    import mlflow\n",
    "    import mlflow.models\n",
    "    from mlflow.types.responses import ResponsesAgentRequest\n",
    "    model_config = mlflow.models.ModelConfig(development_config=\"../02_agent_eval/agent_config.yaml\")\n",
    "    with mlflow.start_run(run_name=model_config.get('config_version_name')):\n",
    "        return mlflow.pyfunc.log_model(\n",
    "            name=\"agent\",\n",
    "            python_model=\"../02_agent_eval/agent.py\",\n",
    "            model_config=\"../02_agent_eval/agent_config.yaml\",\n",
    "            input_example=ResponsesAgentRequest(input=[{\"role\": \"user\", \"content\": request_example}]),\n",
    "            resources=resources, # Determine Databricks resources (endpoints, fonctions, vs...) to specify for automatic auth passthrough at deployment time\n",
    "            extra_pip_requirements=[\"databricks-connect\"]\n",
    "        )\n",
    "\n",
    "def predict_wrapper(question):\n",
    "    # Format for chat-style models\n",
    "    model_input = pd.DataFrame({\n",
    "        \"input\": [[{\"role\": \"user\", \"content\": question}]]\n",
    "    })\n",
    "    response = loaded_model.predict(model_input)\n",
    "    return response['output'][-1]['content'][-1]['text']\n",
    "\n",
    "\n",
    "\n",
    "def get_scorers():\n",
    "    from mlflow.genai.scorers import RetrievalGroundedness, RelevanceToQuery, Safety, Guidelines\n",
    "    return [\n",
    "        RetrievalGroundedness(),  # Checks if email content is grounded in retrieved data\n",
    "        RelevanceToQuery(),  # Checks if email addresses the user's request\n",
    "        Safety(),  # Checks for harmful or inappropriate content\n",
    "        Guidelines(\n",
    "            guidelines=\"\"\"Reponse must be done without showing reaso\n",
    "            ning.\n",
    "            - don't mention that you need to look up things\n",
    "            - do not mention tools or function used\n",
    "            - do not tell your intermediate steps or reasoning\"\"\",\n",
    "            name=\"steps_and_reasoning\",\n",
    "        )\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e709c27-1f25-4fca-899a-4d2680bb618c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def endpoint_exists(vsc, vs_endpoint_name):\n",
    "  try:\n",
    "    return vs_endpoint_name in [e['name'] for e in vsc.list_endpoints().get('endpoints', [])]\n",
    "  except Exception as e:\n",
    "    #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "    if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "      print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. The demo will consider it exists\")\n",
    "      return True\n",
    "    else:\n",
    "      raise e\n",
    "\n",
    "def wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name):\n",
    "  for i in range(180):\n",
    "    try:\n",
    "      endpoint = vsc.get_endpoint(vs_endpoint_name)\n",
    "    except Exception as e:\n",
    "      #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "      if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "        print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. Please manually check your endpoint status\")\n",
    "        return\n",
    "      else:\n",
    "        raise e\n",
    "    status = endpoint.get(\"endpoint_status\", endpoint.get(\"status\"))[\"state\"].upper()\n",
    "    if \"ONLINE\" in status:\n",
    "      return endpoint\n",
    "    elif \"PROVISIONING\" in status or i <6:\n",
    "      if i % 20 == 0: \n",
    "        print(f\"Waiting for endpoint to be ready, this can take a few min... {endpoint}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "      raise Exception(f'''Error with the endpoint {vs_endpoint_name}. - this shouldn't happen: {endpoint}.\\n Please delete it and re-run the previous cell: vsc.delete_endpoint(\"{vs_endpoint_name}\")''')\n",
    "  raise Exception(f\"Timeout, your endpoint isn't ready yet: {vsc.get_endpoint(vs_endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14cf480b-2ed5-4cad-9707-0d13d5ff579c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def index_exists(vsc, endpoint_name, index_full_name):\n",
    "    try:\n",
    "        vsc.get_index(endpoint_name, index_full_name).describe()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if 'RESOURCE_DOES_NOT_EXIST' not in str(e):\n",
    "            print(f'Unexpected error describing the index. This could be a permission issue.')\n",
    "            raise e\n",
    "    return False\n",
    "    \n",
    "def wait_for_index_to_be_ready(vsc, vs_endpoint_name, index_name):\n",
    "  for i in range(180):\n",
    "    idx = vsc.get_index(vs_endpoint_name, index_name).describe()\n",
    "    index_status = idx.get('status', idx.get('index_status', {}))\n",
    "    status = index_status.get('detailed_state', index_status.get('status', 'UNKNOWN')).upper()\n",
    "    url = index_status.get('index_url', index_status.get('url', 'UNKNOWN'))\n",
    "    if \"ONLINE\" in status:\n",
    "      return\n",
    "    if \"UNKNOWN\" in status:\n",
    "      print(f\"Can't get the status - will assume index is ready {idx} - url: {url}\")\n",
    "      return\n",
    "    elif \"PROVISIONING\" in status:\n",
    "      if i % 40 == 0: print(f\"Waiting for index to be ready, this can take a few min... {index_status} - pipeline url:{url}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "        raise Exception(f'''Error with the index - this shouldn't happen. DLT pipeline might have been killed.\\n Please delete it and re-run the previous cell: vsc.delete_index(\"{index_name}, {vs_endpoint_name}\") \\nIndex details: {idx}''')\n",
    "  raise Exception(f\"Timeout, your index isn't ready yet: {vsc.get_index(index_name, vs_endpoint_name)}\")\n",
    "\n",
    "def wait_for_model_serving_endpoint_to_be_ready(ep_name):\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate\n",
    "    import time\n",
    "\n",
    "    # TODO make the endpoint name as a param\n",
    "    # Wait for it to be ready\n",
    "    w = WorkspaceClient()\n",
    "    state = \"\"\n",
    "    for i in range(200):\n",
    "        state = w.serving_endpoints.get(ep_name).state\n",
    "        if state.config_update == EndpointStateConfigUpdate.IN_PROGRESS:\n",
    "            if i % 40 == 0:\n",
    "                print(f\"Waiting for endpoint to deploy {ep_name}. Current state: {state}\")\n",
    "            time.sleep(10)\n",
    "        elif state.ready == EndpointStateReady.READY:\n",
    "          print('endpoint ready.')\n",
    "          return\n",
    "        else:\n",
    "          break\n",
    "    raise Exception(f\"Couldn't start the endpoint, timeout, please check your endpoint for more details: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f623468-569a-4d7e-a850-2adf939fd269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.exceptions import RestException\n",
    "\n",
    "# Save original function\n",
    "_original_set_experiment = mlflow.set_experiment\n",
    "\n",
    "# Define patched version\n",
    "def patched_set_experiment(experiment_name_or_path):\n",
    "    try:\n",
    "        _original_set_experiment(experiment_name_or_path)\n",
    "    except RestException as e:\n",
    "        if \"experiment creation is not permitted in a repo\" in str(e):\n",
    "            from databricks.sdk import WorkspaceClient\n",
    "            fallback_path = \"/Shared/dbdemos/ai-agent\"\n",
    "            fallback_path_xp = fallback_path+\"/ai_agent_experiment\"\n",
    "            w = WorkspaceClient()\n",
    "            w.workspace.mkdirs(fallback_path)\n",
    "            print(f\"mlflow.set_experiment('{experiment_name_or_path}') failed due to repo path (probably as job).\")\n",
    "            print(f\"Falling back to: {fallback_path} - see code patched in _resource/01-setup\")\n",
    "            \n",
    "            # Create fallback experiment if it doesn't exist\n",
    "            try:\n",
    "                mlflow.get_experiment_by_name(fallback_path_xp) or mlflow.create_experiment(fallback_path_xp)\n",
    "            except Exception as create_err:\n",
    "                print(f\"❌ Failed to create fallback experiment: {create_err}\")\n",
    "                raise create_err\n",
    "\n",
    "            # Call original on fallback\n",
    "            _original_set_experiment(fallback_path)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Apply monkey patch\n",
    "mlflow.set_experiment = patched_set_experiment"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
