{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bc19aaf-d296-4afe-819d-95f079068e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 2 - Evaluating and Deploying our Agent Systems\n",
    "\n",
    "We are going to pacakge our tools together using Langchain (in the agent.py file), and use it as a first agent version to run our evaluation!\n",
    "\n",
    "## Agent Evaluation with MLFlow 3\n",
    "Now that we've created an agent, we need to measure its performance, and find a way to compare it with previous versions.\n",
    "\n",
    "Databricks makes it very easy with MLFlow 3. You can automatically:\n",
    "\n",
    "- Trace all your agent input/output\n",
    "- Capture end user feedback\n",
    "- Evaluate your agent against custom or synthetic evaluation dataset\n",
    "- Build labeled dataset with your business expert\n",
    "- Compare each evaluation against the previous one\n",
    "- Deploy and track your evaluations once deployed in production \n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/ai-agent/mlflow-evaluate-0.png?raw=true?raw=true\" width=\"800px\">\n",
    "\n",
    "### Our agent is composed of:\n",
    "\n",
    "- [**agent.py**]($./agent.py): in this file, we used Langchain to prepare an agent ready to be used.\n",
    "- [**agent_config.yaml**]($./agent_config.yaml): this file contains our agent configuration, including the system prompt and the LLM endpoint that we'll use\n",
    "\n",
    "Let's get started and try our Langchain agent in this notebook!\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=796524194907820&notebook=%2F02-agent-eval%2F02.1_agent_evaluation&demo_name=ai-agent&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fai-agent%2F02-agent-eval%2F02.1_agent_evaluation&version=1\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ff99abf-8546-4f8f-8e49-b6970d70168e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq mlflow>=3.1.4 langchain langgraph databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] uv databricks-feature-engineering==0.12.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dc689a3-0ff0-4ed8-a74c-4855f8f60790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/01-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01937e86-96bf-4b6f-81f1-6602f1e88e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1/ Build and register our agent\n",
    "\n",
    "### 1.1/ Define our agent configuration\n",
    "Let's first update our configuration file with the tools we want our langchain agent to use, a basic system prompt and the endpoint we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90afb717-d829-4184-9781-db405427b19e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "update our configuration file"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import mlflow\n",
    "\n",
    "rag_chain_config = {\n",
    "    \"config_version_name\": \"first_config\",\n",
    "    \"input_example\": [{\"role\": \"user\", \"content\": \"Give me the orders for john21@example.net\"}],\n",
    "    \"uc_tool_names\": [f\"{catalog}.{dbName}.*\"],\n",
    "    \"system_prompt\": \"Your job is to provide customer help. call the tool to answer.\",\n",
    "    \"llm_endpoint_name\": LLM_ENDPOINT_NAME,\n",
    "    \"max_history_messages\": 20,\n",
    "    \"retriever_config\": None\n",
    "}\n",
    "try:\n",
    "    with open('agent_config.yaml', 'w') as f:\n",
    "        yaml.dump(rag_chain_config, f)\n",
    "except:\n",
    "    print('pass to work on build job')\n",
    "model_config = mlflow.models.ModelConfig(development_config='agent_config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3a587a6-dea2-485e-8ffb-1465fe51e5da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We created our AGENT using langchain in the `agent.py` file. You can explore it to see the code behind the scene.\n",
    "\n",
    "In this notebook, we'll keep it simple and just import it and send a request to explore its internal tracing with MLFlow Trace UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89c67b77-3d31-41bc-8656-6ff9709d3991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "# Correct request format\n",
    "request_example = \"Give me the information about john21@example.net\"\n",
    "answer = AGENT.predict({\"input\":[{\"role\": \"user\", \"content\": request_example}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "7b118a0e-9df9-475f-ac17-0b4601bf8404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 1.2/ Open MLFlow tracing\n",
    "\n",
    "<img src=\"https://i.imgur.com/tNYUHdC.gif\" style=\"float: right\" width=\"700px\">\n",
    "\n",
    "Open now the experiment from the right notebook menu. You'll see in the traces the message we just sent: `Give me the information about john21@example.net`.\n",
    "\n",
    "MLFlow keeps track of all the input/output and internal tracing so that we can analyze existing request, and create better evaluation dataset over time!\n",
    "\n",
    "Not only MLFlow traces all your agent request, but you can also easily capture end-users feedback to quickly detect which answer was wrong and improve your agent accordingly! \n",
    "\n",
    "*We'll show you how to capture feedback when we'll deploy the application!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bfd0f38-b612-4c12-a169-d359efcba7ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.3/ Log the `agent` as an MLflow model\n",
    "\n",
    "This looks good! Let's log the agent in our MLFlow registry using the [agent]($./agent) python file to avoid any serialization issue. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code).\n",
    "\n",
    "*Note that we'll also pass the list of Databricks resources (functions, warehouse etc) that our agent need to use to properly work. This will handle the permissions for us during its deployment!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f891c2ca-6a30-4266-9639-147b4c6abc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "def log_customer_support_agent_model(resources, request_example):\n",
    "    with mlflow.start_run(run_name=model_config.get('config_version_name')):\n",
    "        return mlflow.pyfunc.log_model(\n",
    "            name=\"agent\",\n",
    "            python_model=\"agent.py\",\n",
    "            model_config=\"agent_config.yaml\",\n",
    "            input_example={\"input\": [{\"role\": \"user\", \"content\": request_example}]},\n",
    "            resources=resources, # Determine Databricks resources (endpoints, fonctions, vs...) to specify for automatic auth passthrough at deployment time\n",
    "            extra_pip_requirements=[\"databricks-connect\"]\n",
    "        )\n",
    "logged_agent_info = log_customer_support_agent_model(AGENT.get_resources(), request_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13a3f799-24bb-4e1f-b9eb-384faf6da9fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.4/ Let's load and try our model\n",
    "Our model is saved on MLFlow! Let's load it and give it a try. We'll wrap our predict function so that we can extract more easily the final answer, and also make our evaluation easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3d71e5e-237f-4d5c-811e-3cedd4c7786c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the model and create a prediction function\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"runs:/{logged_agent_info.run_id}/agent\")\n",
    "def predict_wrapper(question):\n",
    "    # Format for chat-style models\n",
    "    model_input = pd.DataFrame({\n",
    "        \"input\": [[{\"role\": \"user\", \"content\": question}]]\n",
    "    })\n",
    "    response = loaded_model.predict(model_input)\n",
    "    return response['output'][-1]['content'][-1]['text']\n",
    "\n",
    "answer = predict_wrapper(\"Give me the orders for john21@example.net.\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fb31d0a-fcba-4d14-ad60-e0867d3d7417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2/ Evaluation\n",
    "\n",
    "### 2.1/ Evaluate the agent with [Agent Evaluation](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/)\n",
    "\n",
    "We prepared an evaluation dataset ready to use as part of this demo. However, multiple strateges exist:\n",
    "\n",
    "- create your own evaluation dataset (what we'll do)\n",
    "- use existing traces from MLFlow and add them to your dataset (from the API or your experiment UI)\n",
    "- use Databricks genai eval synthetic dataset creation (see the [PDF RAG notebook]($../03-knowledge-base-rag/03.1-pdf-rag-tool) for an example)\n",
    "- Create labeling session where you can get insights from expert, using the MLFlow UI directly!\n",
    "\n",
    "Note: you can also select the existing LLM call from the traces and add to your eval dataset with the UI, or use the API directly:\n",
    "\n",
    "```\n",
    "traces = mlflow.search_traces(filter_string=f\"attributes.timestamp_ms > {ten_minutes_ago} AND attributes.status = 'OK'\", order_by=[\"attributes.timestamp_ms DESC\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa869297-4e91-403d-b952-cdd8e83e434a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/04-eval-dataset-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e96c0ddf-7f3b-4e6b-a1a1-11459f0bb0e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_example = spark.read.json(f\"/Volumes/{catalog}/{dbName}/{volume_name}/eval_dataset\")\n",
    "display(eval_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2fa6e6-6a25-4b98-89de-a56ab666ca3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2/ Create our MLFlow dataset\n",
    "Let's use the API to create our dataset. You can also directly do it from the Experiment UI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "797df7ac-fb6b-4a9c-92a9-4a27fc9c1150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "\n",
    "eval_dataset_table_name = f\"{catalog}.{dbName}.ai_agent_mlflow_eval\"\n",
    "\n",
    "try:\n",
    "  eval_dataset = mlflow.genai.datasets.get_dataset(eval_dataset_table_name)\n",
    "except Exception as e:\n",
    "  if 'does not exist' in str(e):\n",
    "    eval_dataset = mlflow.genai.datasets.create_dataset(eval_dataset_table_name)\n",
    "    # Add your examples to the evaluation dataset\n",
    "    eval_dataset.merge_records(eval_example)\n",
    "    print(\"Added records to the evaluation dataset.\")\n",
    "\n",
    "# Preview the dataset\n",
    "display(eval_dataset.to_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "ea7a1424-82e5-4aa9-9f76-4578a2b865f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.3/ Adding guidelines to track our agent behavior\n",
    "\n",
    "<img src=\"https://i.imgur.com/M3kLBHF.gif\" style=\"float:right\" width=\"700px\">\n",
    "\n",
    "MLFlow 3.0 lets you create custom guidelines to evaluate your agent behavior.\n",
    "\n",
    "We'll use a few of the built-in one, and add a custome `Guidelines` on steps and reasoning: we want our LLM to output the answer without mentioning the internal tools it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c38413b-bd6d-4d07-b20b-57ed51725856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import RetrievalGroundedness, RelevanceToQuery, Safety, Guidelines\n",
    "\n",
    "def get_scorers():\n",
    "    return [\n",
    "        RetrievalGroundedness(),  # Checks if email content is grounded in retrieved data\n",
    "        RelevanceToQuery(),  # Checks if email addresses the user's request\n",
    "        Safety(),  # Checks for harmful or inappropriate content\n",
    "        Guidelines(\n",
    "            guidelines=\"\"\"\n",
    "            Reponse must be done without showing reasoning.\n",
    "            - don't mention that you need to look up things\n",
    "            - do not mention tools or function used\n",
    "            - do not tell your intermediate steps or reasoning\n",
    "            \"\"\",\n",
    "            name=\"steps_and_reasoning\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "scorers = get_scorers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6943bb0-232d-4b22-a56a-46614bd8b68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.4/ Run the evaluations against our guidelines\n",
    "\n",
    "That's it, let's now evaluate our dataset with our guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d1734a-9dc0-4622-815f-38071a9bece5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='eval_with_no_reasoning_instructions'):\n",
    "    results = mlflow.genai.evaluate(data=eval_dataset, predict_fn=predict_wrapper, scorers=scorers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aca56e1-f8e3-4e72-9750-2852bbbd43fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3/ Improving our eval metrics with a better system prompt\n",
    "\n",
    "As we can see in the eval, the agent emits a lot of information on the internal tools and steps. \n",
    "For example; it would mention things like:\n",
    "\n",
    "`First, I need to find his customer record using his email address. Since I don't have Thomas Green's email address yet, I need to ask for it.`\n",
    "\n",
    "While this is good reasoning, we do not want this in the final answer!\n",
    "\n",
    "### 3.1/ Deploying a new model version with a better system prompt\n",
    "\n",
    "Let's update our system prompt with better instruction to avoid this behavior, and run our eval to make sure this improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63db3401-8a53-43a4-b3a5-61e2f35c6b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    config = yaml.safe_load(open(\"agent_config.yaml\"))\n",
    "    config[\"config_version_name\"] = \"better_prompt\"\n",
    "    config[\"system_prompt\"] = (\n",
    "        \"You are a telco assistant. Call the appropriate tool to help the user with billing, support, or account info. \"\n",
    "        \"DO NOT mention any internal tool or reasoning steps in your final answer. Do not say according to records or imply that you are looking up information.\"\n",
    "    )\n",
    "    yaml.dump(config, open(\"agent_config.yaml\", \"w\"))\n",
    "except Exception as e:\n",
    "    print(f\"Skipped update - ignore for job run - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b39e910-d026-4e6e-b395-fd5f9101a95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's relog our agent in MLflow to capture the new prompt\n",
    "logged_agent_info = log_customer_support_agent_model(AGENT.get_resources(), request_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "711f6427-98e1-4569-92fa-e3699e47bdaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the model to be used in evaluation via `predict_wrapper`\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"runs:/{logged_agent_info.run_id}/agent\")\n",
    "\n",
    "with mlflow.start_run(run_name='eval_with_reasoning_instructions'):\n",
    "    results = mlflow.genai.evaluate(data=eval_dataset, predict_fn=predict_wrapper, scorers=scorers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fb9bf0b-ed7b-4859-a4b4-df284836388e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Open your experiment and check the results!\n",
    "\n",
    "Select the previous run and this one, and compare them. You should see some improvements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "268972e7-b1ef-46b5-86fd-9eec4657d9c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4/ Deploy our agent as an endpoint!\n",
    "\n",
    "Everything looks good! Our latest version now has decent eval score. Let's deploy it as a realtime endpoint for our end user chat application.\n",
    "\n",
    "### 4.1/ Register our new model version to Unity Catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16f24471-4a22-46f0-9f37-c55357daaa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "UC_MODEL_NAME = f\"{catalog}.{dbName}.{MODEL_NAME}\"\n",
    "\n",
    "# register the model to UC\n",
    "client = MlflowClient()\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME, tags={\"model\": \"customer_support_agent\"})\n",
    "client.set_registered_model_alias(name=UC_MODEL_NAME, alias=\"model-to-deploy\", version=uc_registered_model_info.version)\n",
    "\n",
    "# Create HTML link to created agent\n",
    "displayHTML(f'<a href=\"/explore/data/models/{catalog}/{dbName}/{MODEL_NAME}\" target=\"_blank\">Open Unity Catalog to see Registered Agent</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d85cd7d-7b86-4ee2-994a-1962b2f51481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2/ Deploy the agent\n",
    "\n",
    "Let's now start our model endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b0e0eab-e714-4bec-a224-0561f201db51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "# Deploy the model to the review app and a model serving endpoint\n",
    "if len(agents.get_deployments(model_name=UC_MODEL_NAME, model_version=uc_registered_model_info.version)) == 0:\n",
    "  agents.deploy(UC_MODEL_NAME, uc_registered_model_info.version, endpoint_name=ENDPOINT_NAME, tags = {\"project\": \"dbdemos\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3979908-d8ea-47d5-b9a3-392a3ebe2f01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next: adding a tool to answer questions about our knowledge base (RAG + Vector Search on PDF)\n",
    "\n",
    "Our model is working well, but it can't answer specific questions that our customer support might have about their subscription.\n",
    "\n",
    "For example, if we ask our Agent how to solve a specific error code in our WIFI router, it'll fail as it doesn't have any valuable information about it.\n",
    "\n",
    "Open the [03-knowledge-base-rag/03.1-pdf-rag-tool]($../03-knowledge-base-rag/03.1-pdf-rag-tool)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "02.1_agent_evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
