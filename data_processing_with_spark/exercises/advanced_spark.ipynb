{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757eee14",
   "metadata": {},
   "source": [
    "## Spark Session Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c31784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced Spark\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5b740",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c010c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: long (nullable = true)\n",
      " |-- last_scraped: timestamp (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: timestamp (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_acceptance_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_neighbourhood: string (nullable = true)\n",
      " |-- host_listings_count: integer (nullable = true)\n",
      " |-- host_total_listings_count: integer (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- neighbourhood_group_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- minimum_minimum_nights: integer (nullable = true)\n",
      " |-- maximum_minimum_nights: integer (nullable = true)\n",
      " |-- minimum_maximum_nights: integer (nullable = true)\n",
      " |-- maximum_maximum_nights: integer (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: double (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: double (nullable = true)\n",
      " |-- calendar_updated: string (nullable = true)\n",
      " |-- has_availability: string (nullable = true)\n",
      " |-- availability_30: integer (nullable = true)\n",
      " |-- availability_60: integer (nullable = true)\n",
      " |-- availability_90: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- calendar_last_scraped: timestamp (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- number_of_reviews_l30d: integer (nullable = true)\n",
      " |-- availability_eoy: integer (nullable = true)\n",
      " |-- number_of_reviews_ly: integer (nullable = true)\n",
      " |-- estimated_occupancy_l365d: integer (nullable = true)\n",
      " |-- estimated_revenue_l365d: integer (nullable = true)\n",
      " |-- first_review: timestamp (nullable = true)\n",
      " |-- last_review: timestamp (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: integer (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings = spark.read.csv(\"../data/listings.csv.gz\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\", \n",
    "    quote='\"',\n",
    "    escape='\"', \n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\" \n",
    ")\n",
    "listings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb66d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- reviewer_id: integer (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.csv(\"../data/reviews.csv.gz\", \n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\"\n",
    ")\n",
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994c7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. For each listing compute string category depending on its price, and add it as a new column.\n",
    "# A category is defined in the following way:\n",
    "#\n",
    "# * price < 50 -> \"Budget\"\n",
    "# * 50 <= price < 150 -> \"Mid-range\"\n",
    "# * price >= 150 -> \"Luxury\"\n",
    "# \n",
    "# Only include listings where the price is not null.\n",
    "# Count the number of listings in each category\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "listings = listings.withColumn('price_numeric', regexp_replace('price', '[$,]', '').cast('float'))\n",
    "\n",
    "def categorize_price(price):\n",
    "    if price is None:\n",
    "        return 'Unknown'\n",
    "    elif price < 50:\n",
    "        return 'Budget'\n",
    "    elif 50 <= price < 150:\n",
    "        return 'Mid-range'\n",
    "    elif price >= 150:\n",
    "        return 'Luxury'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "\n",
    "categorize_price_udf = udf(categorize_price, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35dbb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_with_category = listings \\\n",
    "  .filter(listings.price_numeric.isNotNull()) \\\n",
    "  .withColumn(\n",
    "    'price_category',\n",
    "    categorize_price_udf(listings.price_numeric)\n",
    "  ) \\\n",
    "  .groupBy('price_category') \\\n",
    "  .count() \\\n",
    "  .orderBy('count', ascending=False) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de7e4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. In this task you will need to compute a santiment score per review, and then an average sentiment score per listing.\n",
    "# A santiment score indicates how \"positive\" or \"negative\" a review is. The higher the score the more positive it is, and vice-versa.\n",
    "#\n",
    "# To compute a sentiment score per review compute the number of positive words in a review and subtract the number of negative\n",
    "# words in the same review (the list of words is already provided)\n",
    "#\n",
    "# To complete this task, compute a DataFrame that contains the following fields:\n",
    "# * name - the name of a listing\n",
    "# * average_sentiment - average sentiment of reviews computed using the algorithm described above\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "positive_words = ['good', 'great', 'excellent', 'amazing', 'fantastic', 'wonderful', 'pleasant', 'lovely', 'nice', 'enjoyed']\n",
    "negative_words = ['bad', 'terrible', 'awful', 'horrible', 'disappointing', 'poor', 'hate', 'unpleasant', 'dirty', 'noisy']\n",
    "\n",
    "def sentiment_score(comment):\n",
    "    if comment is None:\n",
    "        return 0.0\n",
    "    comment_lower = comment.lower()\n",
    "    score = 0\n",
    "\n",
    "    for word in positive_words:\n",
    "        if word in comment_lower:\n",
    "            score += 1\n",
    "\n",
    "    for word in negative_words:\n",
    "        if word in comment_lower:\n",
    "            score -= 1\n",
    "    return float(score)\n",
    "\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "034c285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 2.0\n"
     ]
    }
   ],
   "source": [
    "test_comment = \"This place was great and wonderful!\"\n",
    "result = sentiment_score(test_comment)\n",
    "print(f\"Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded15e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- reviewer_id: integer (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca0874a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"c:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\serializers.py:458\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pickle.PickleError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, protocol, buffer_callback)\u001b[39m\n\u001b[32m     70\u001b[39m cp = CloudPickler(\n\u001b[32m     71\u001b[39m     file, protocol=protocol, buffer_callback=buffer_callback\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:602\u001b[39m, in \u001b[36mCloudPickler.dump\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:692\u001b[39m, in \u001b[36mCloudPickler.reducer_override\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types.FunctionType):\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    694\u001b[39m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[32m    695\u001b[39m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:565\u001b[39m, in \u001b[36mCloudPickler._function_reduce\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:546\u001b[39m, in \u001b[36mCloudPickler._dynamic_function_reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    545\u001b[39m newargs = \u001b[38;5;28mself\u001b[39m._function_getnewargs(func)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m state = \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (types.FunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    548\u001b[39m         _function_setstate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:157\u001b[39m, in \u001b[36m_function_getstate\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    146\u001b[39m slotstate = {\n\u001b[32m    147\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__name__\u001b[39m,\n\u001b[32m    148\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__qualname__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__closure__\u001b[39m\u001b[33m\"\u001b[39m: func.\u001b[34m__closure__\u001b[39m,\n\u001b[32m    155\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m f_globals_ref = \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m f_globals = {k: func.\u001b[34m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    159\u001b[39m              func.\u001b[34m__globals__\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:334\u001b[39m, in \u001b[36m_extract_code_globals\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m out_names = {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m reviews_with_sentiment = reviews \\\n\u001b[32m      2\u001b[39m   .withColumn(\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msentiment_score\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43msentiment_score_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreviews\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m   )\n\u001b[32m      7\u001b[39m listings \\\n\u001b[32m      8\u001b[39m    .join(reviews_with_sentiment, listings.id == reviews.listing_id, \u001b[33m'\u001b[39m\u001b[33minner\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m      9\u001b[39m    .groupBy(\u001b[33m'\u001b[39m\u001b[33mlisting_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m    .select(\u001b[33m'\u001b[39m\u001b[33mlisting_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33maverage_sentiment\u001b[39m\u001b[33m'\u001b[39m) \\\n\u001b[32m     15\u001b[39m    .show(truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\udf.py:276\u001b[39m, in \u001b[36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(\u001b[38;5;28mself\u001b[39m.func, assigned=assignments)\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\udf.py:249\u001b[39m, in \u001b[36mUserDefinedFunction.__call__\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m    247\u001b[39m     judf = \u001b[38;5;28mself\u001b[39m._create_judf(func)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     judf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_judf\u001b[49m\n\u001b[32m    251\u001b[39m jPythonUDF = judf.apply(_to_seq(sc, cols, _to_java_column))\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\udf.py:215\u001b[39m, in \u001b[36mUserDefinedFunction._judf\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_judf\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> JavaObject:\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# It is possible that concurrent access, to newly created UDF,\u001b[39;00m\n\u001b[32m    211\u001b[39m     \u001b[38;5;66;03m# will initialize multiple UserDefinedPythonFunctions.\u001b[39;00m\n\u001b[32m    212\u001b[39m     \u001b[38;5;66;03m# This is unlikely, doesn't affect correctness,\u001b[39;00m\n\u001b[32m    213\u001b[39m     \u001b[38;5;66;03m# and should have a minimal performance impact.\u001b[39;00m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._judf_placeholder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m         \u001b[38;5;28mself\u001b[39m._judf_placeholder = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_judf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._judf_placeholder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\udf.py:224\u001b[39m, in \u001b[36mUserDefinedFunction._create_judf\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    221\u001b[39m spark = SparkSession._getActiveSessionOrCreate()\n\u001b[32m    222\u001b[39m sc = spark.sparkContext\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m wrapped_func = \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m jdt = spark._jsparkSession.parseDataType(\u001b[38;5;28mself\u001b[39m.returnType.json())\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\udf.py:50\u001b[39m, in \u001b[36m_wrap_function\u001b[39m\u001b[34m(sc, func, returnType)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_function\u001b[39m(\n\u001b[32m     47\u001b[39m     sc: SparkContext, func: Callable[..., Any], returnType: \u001b[33m\"\u001b[39m\u001b[33mDataTypeOrString\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m ) -> JavaObject:\n\u001b[32m     49\u001b[39m     command = (func, returnType)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     pickled_command, broadcast_vars, env, includes = \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sc._jvm.PythonFunction(\n\u001b[32m     53\u001b[39m         \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[32m     54\u001b[39m         env,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m         sc._javaAccumulator,\n\u001b[32m     60\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:3345\u001b[39m, in \u001b[36m_prepare_for_python_RDD\u001b[39m\u001b[34m(sc, command)\u001b[39m\n\u001b[32m   3342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_for_python_RDD\u001b[39m(sc: \u001b[33m\"\u001b[39m\u001b[33mSparkContext\u001b[39m\u001b[33m\"\u001b[39m, command: Any) -> Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[32m   3343\u001b[39m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[32m   3344\u001b[39m     ser = CloudPickleSerializer()\n\u001b[32m-> \u001b[39m\u001b[32m3345\u001b[39m     pickled_command = \u001b[43mser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) > sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[32m   3348\u001b[39m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\serializers.py:468\u001b[39m, in \u001b[36mCloudPickleSerializer.dumps\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    466\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, emsg)\n\u001b[32m    467\u001b[39m print_exec(sys.stderr)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m pickle.PicklingError(msg)\n",
      "\u001b[31mPicklingError\u001b[39m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "reviews_with_sentiment = reviews \\\n",
    "  .withColumn(\n",
    "    'sentiment_score',\n",
    "    sentiment_score_udf(reviews.comments)\n",
    "  )\n",
    "\n",
    "listings \\\n",
    "   .join(reviews_with_sentiment, listings.id == reviews.listing_id, 'inner') \\\n",
    "   .groupBy('listing_id', 'name') \\\n",
    "   .agg(\n",
    "      avg('sentiment_score').alias('average_sentiment')\n",
    "   ) \\\n",
    "   .orderBy('average_sentiment', ascending=False) \\\n",
    "   .select('listing_id', 'name', 'average_sentiment') \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dd5730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+-----------+-------------+--------------------+---------------+\n",
      "|listing_id|       id|               date|reviewer_id|reviewer_name|            comments|sentiment_score|\n",
      "+----------+---------+-------------------+-----------+-------------+--------------------+---------------+\n",
      "|     13913|    80770|2010-08-18 00:00:00|     177109|      Michael|My girlfriend and...|            1.0|\n",
      "|     13913|   367568|2011-07-11 00:00:00|   19835707|      Mathias|Alina was a reall...|            1.0|\n",
      "|     13913|   529579|2011-09-13 00:00:00|    1110304|      Kristin|Alina is an amazi...|            1.0|\n",
      "|     13913|   595481|2011-10-03 00:00:00|    1216358|      Camilla|Alina's place is ...|            1.0|\n",
      "|     13913|   612947|2011-10-09 00:00:00|     490840|        Jorik|Nice location in ...|            1.0|\n",
      "|     13913|  4847959|2013-05-28 00:00:00|    6405442|         Vera|I'm very happy to...|            1.0|\n",
      "|     13913|  8142329|2013-10-17 00:00:00|    9195551|         Honi|I stayed with Ali...|            1.0|\n",
      "|     13913| 11876590|2014-04-17 00:00:00|    5194009|   Alessandro|Alina was a perfe...|            1.0|\n",
      "|     13913| 46669566|2015-09-12 00:00:00|   42970248|         Oleh|Alina's flat is e...|            1.0|\n",
      "|     13913| 64559033|2016-03-05 00:00:00|   45337884|           Mo|The House is a pi...|            0.0|\n",
      "|     13913|196153582|2017-09-22 00:00:00|   40946748|            A|Was great base fo...|            1.0|\n",
      "|     13913|261305314|2018-05-06 00:00:00|  106481379|       Daniel|Alina was an amaz...|            1.0|\n",
      "|     13913|277816069|2018-06-17 00:00:00|   31960932|      Belinda|Lovely relaxed pl...|            1.0|\n",
      "|     13913|451955791|2019-05-12 00:00:00|   58728173|      Charles|Alina's place is ...|            0.0|\n",
      "|     13913|467269212|2019-06-10 00:00:00|    2291517|       Andrew|Alina, was very q...|            0.0|\n",
      "|     13913|538005731|2019-09-29 00:00:00|    7253695|         Bart|Alina is an amazi...|            1.0|\n",
      "|     13913|539957261|2019-10-02 00:00:00|   45592945|         John|Alina is a very r...|            1.0|\n",
      "|     13913|543287825|2019-10-07 00:00:00|   28531625|       Philip|Felt at home - Al...|            1.0|\n",
      "|     13913|568976830|2019-11-25 00:00:00|   38999586|         Ania|Alina’s place is ...|            1.0|\n",
      "|     13913|609079783|2020-02-22 00:00:00|  336060662|       Samuel|Outstanding host....|            1.0|\n",
      "+----------+---------+-------------------+-----------+-------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, lower, length\n",
    "\n",
    "positive_words = ['good', 'great', 'excellent', 'amazing', 'fantastic', 'wonderful', 'pleasant', 'lovely', 'nice', 'enjoyed']\n",
    "negative_words = ['bad', 'terrible', 'awful', 'horrible', 'disappointing', 'poor', 'hate', 'unpleasant', 'dirty', 'noisy']\n",
    "\n",
    "# Avoid UDF completely - use built-in functions\n",
    "# Create regex patterns for positive and negative words\n",
    "positive_pattern = '|'.join([f'\\\\b{word}\\\\b' for word in positive_words])\n",
    "negative_pattern = '|'.join([f'\\\\b{word}\\\\b' for word in negative_words])\n",
    "\n",
    "reviews_with_sentiment = reviews.limit(20) \\\n",
    "  .withColumn(\n",
    "    'sentiment_score',\n",
    "    when(col('comments').isNull(), 0.0)\n",
    "    .when(lower(col('comments')).rlike(positive_pattern), 1.0)\n",
    "    .when(lower(col('comments')).rlike(negative_pattern), -1.0)\n",
    "    .otherwise(0.0)\n",
    "  )\n",
    "\n",
    "reviews_with_sentiment.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
